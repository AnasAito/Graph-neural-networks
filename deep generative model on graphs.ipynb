{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DGM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUZUJKdWE1r4AjXcvICgsm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnasAito/Graph-neural-networks/blob/main/deep%20generative%20model%20on%20graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUvfZq177G3D",
        "outputId": "84002078-4a83-468e-dfce-28237afc7e3a"
      },
      "source": [
        "# link to paper : https://arxiv.org/pdf/1803.03324.pdf\n",
        "# link to original code https://github.com/dmlc/dgl/blob/master/examples/pytorch/dgmg\n",
        "!pip install dgl"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/72/61668fa3ef059f889f98653197e6f93592735660298042ffc5adb2005ca5/dgl-0.6.0.post1-cp37-cp37m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.0.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lolDBm2jQ2GC"
      },
      "source": [
        "# configure.py\n",
        "\"\"\"We intend to make our reproduction as close as possible to the original paper.\n",
        "The configuration in the file is mostly from the description in the original paper\n",
        "and will be loaded when setting up.\"\"\"\n",
        "\n",
        "\n",
        "def dataset_based_configure(opts):\n",
        "\n",
        "    if opts['dataset'] == 'cycles':\n",
        "        ds_configure = cycles_configure\n",
        "    else:\n",
        "        raise ValueError('Unsupported dataset: {}'.format(opts['dataset']))\n",
        "\n",
        "    opts = {**opts, **ds_configure}\n",
        "\n",
        "    return opts\n",
        "\n",
        "\n",
        "synthetic_dataset_configure = {\n",
        "    'node_hidden_size': 16,\n",
        "    'num_propagation_rounds': 3,\n",
        "    'optimizer': 'Adam',\n",
        "    'nepochs': 25,\n",
        "    'ds_size': 4000,\n",
        "    'num_generated_samples': 100,\n",
        "}\n",
        "\n",
        "cycles_configure = {\n",
        "    **synthetic_dataset_configure,\n",
        "    **{\n",
        "        'min_size': 10,\n",
        "        'max_size': 20,\n",
        "        'lr': 5e-4,\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbMm5K1h6vvL"
      },
      "source": [
        "## cycles.py \n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "def get_previous(i, v_max):\n",
        "    if i == 0:\n",
        "        return v_max\n",
        "    else:\n",
        "        return i - 1\n",
        "\n",
        "\n",
        "def get_next(i, v_max):\n",
        "    if i == v_max:\n",
        "        return 0\n",
        "    else:\n",
        "        return i + 1\n",
        "\n",
        "\n",
        "def is_cycle(g):\n",
        "    size = g.number_of_nodes()\n",
        "\n",
        "    if size < 3:\n",
        "        return False\n",
        "\n",
        "    for node in range(size):\n",
        "        neighbors = g.successors(node)\n",
        "\n",
        "        if len(neighbors) != 2:\n",
        "            return False\n",
        "\n",
        "        if get_previous(node, size - 1) not in neighbors:\n",
        "            return False\n",
        "\n",
        "        if get_next(node, size - 1) not in neighbors:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_decision_sequence(size):\n",
        "    \"\"\"\n",
        "    Get the decision sequence for generating valid cycles with DGMG for teacher\n",
        "    forcing optimization.\n",
        "    \"\"\"\n",
        "    decision_sequence = []\n",
        "\n",
        "    for i in range(size):\n",
        "        decision_sequence.append(0)  # Add node\n",
        "\n",
        "        if i != 0:\n",
        "            decision_sequence.append(0)  # Add edge\n",
        "            decision_sequence.append(i - 1)  # Set destination to be previous node.\n",
        "\n",
        "        if i == size - 1:\n",
        "            decision_sequence.append(0)  # Add edge\n",
        "            decision_sequence.append(0)  # Set destination to be the root.\n",
        "\n",
        "        decision_sequence.append(1)  # Stop adding edge\n",
        "\n",
        "    decision_sequence.append(1)  # Stop adding node\n",
        "\n",
        "    return decision_sequence\n",
        "\n",
        "\n",
        "def generate_dataset(v_min, v_max, n_samples, fname):\n",
        "    samples = []\n",
        "    for _ in range(n_samples):\n",
        "        size = random.randint(v_min, v_max)\n",
        "        print(size)\n",
        "        samples.append(get_decision_sequence(size))\n",
        "    \n",
        "    with open(fname, 'wb') as f:\n",
        "        pickle.dump(samples, f)\n",
        "    #return samples\n",
        "\n",
        "\n",
        "class CycleDataset(Dataset):\n",
        "    def __init__(self, fname):\n",
        "        super(CycleDataset, self).__init__()\n",
        "\n",
        "        with open(fname, 'rb') as f:\n",
        "            self.dataset = pickle.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[index]\n",
        "\n",
        "    def collate_single(self, batch):\n",
        "        assert len(batch) == 1, 'Currently we do not support batched training'\n",
        "        return batch[0]\n",
        "\n",
        "    def collate_batch(self, batch):\n",
        "        return batch\n",
        "\n",
        "\n",
        "def dglGraph_to_adj_list(g):\n",
        "    adj_list = {}\n",
        "    for node in range(g.number_of_nodes()):\n",
        "        # For undirected graph. successors and\n",
        "        # predecessors are equivalent.\n",
        "        adj_list[node] = g.successors(node).tolist()\n",
        "    return adj_list\n",
        "\n",
        "\n",
        "class CycleModelEvaluation(object):\n",
        "    def __init__(self, v_min, v_max, dir):\n",
        "        super(CycleModelEvaluation, self).__init__()\n",
        "\n",
        "        self.v_min = v_min\n",
        "        self.v_max = v_max\n",
        "\n",
        "        self.dir = dir\n",
        "\n",
        "    def rollout_and_examine(self, model, num_samples):\n",
        "        assert not model.training, 'You need to call model.eval().'\n",
        "\n",
        "        num_total_size = 0\n",
        "        num_valid_size = 0\n",
        "        num_cycle = 0\n",
        "        num_valid = 0\n",
        "        plot_times = 0\n",
        "        adj_lists_to_plot = []\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            sampled_graph = model()\n",
        "            if isinstance(sampled_graph, list):\n",
        "                # When the model is a batched implementation, a list of\n",
        "                # DGLGraph objects is returned. Note that with model(),\n",
        "                # we generate a single graph as with the non-batched\n",
        "                # implementation. We actually support batched generation\n",
        "                # during the inference so feel free to modify the code.\n",
        "                sampled_graph = sampled_graph[0]\n",
        "\n",
        "            sampled_adj_list = dglGraph_to_adj_list(sampled_graph)\n",
        "            adj_lists_to_plot.append(sampled_adj_list)\n",
        "\n",
        "            graph_size = sampled_graph.number_of_nodes()\n",
        "            valid_size = (self.v_min <= graph_size <= self.v_max)\n",
        "            cycle = is_cycle(sampled_graph)\n",
        "\n",
        "            num_total_size += graph_size\n",
        "\n",
        "            if valid_size:\n",
        "                num_valid_size += 1\n",
        "\n",
        "            if cycle:\n",
        "                num_cycle += 1\n",
        "\n",
        "            if valid_size and cycle:\n",
        "                num_valid += 1\n",
        "\n",
        "            if len(adj_lists_to_plot) >= 4:\n",
        "                plot_times += 1\n",
        "                fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(2, 2)\n",
        "                axes = {0: ax0, 1: ax1, 2: ax2, 3: ax3}\n",
        "                for i in range(4):\n",
        "                    nx.draw_circular(nx.from_dict_of_lists(adj_lists_to_plot[i]),\n",
        "                                     with_labels=True, ax=axes[i])\n",
        "\n",
        "                plt.savefig(self.dir + '/samples/{:d}'.format(plot_times))\n",
        "                plt.close()\n",
        "\n",
        "                adj_lists_to_plot = []\n",
        "\n",
        "        self.num_samples_examined = num_samples\n",
        "        self.average_size = num_total_size / num_samples\n",
        "        self.valid_size_ratio = num_valid_size / num_samples\n",
        "        self.cycle_ratio = num_cycle / num_samples\n",
        "        self.valid_ratio = num_valid / num_samples\n",
        "\n",
        "    def write_summary(self):\n",
        "\n",
        "        def _format_value(v):\n",
        "            if isinstance(v, float):\n",
        "                return '{:.4f}'.format(v)\n",
        "            elif isinstance(v, int):\n",
        "                return '{:d}'.format(v)\n",
        "            else:\n",
        "                return '{}'.format(v)\n",
        "\n",
        "        statistics = {\n",
        "            'num_samples': self.num_samples_examined,\n",
        "            'v_min': self.v_min,\n",
        "            'v_max': self.v_max,\n",
        "            'average_size': self.average_size,\n",
        "            'valid_size_ratio': self.valid_size_ratio,\n",
        "            'cycle_ratio': self.cycle_ratio,\n",
        "            'valid_ratio': self.valid_ratio\n",
        "        }\n",
        "\n",
        "        model_eval_path = os.path.join(self.dir, 'model_eval.txt')\n",
        "\n",
        "        with open(model_eval_path, 'w') as f:\n",
        "            for key, value in statistics.items():\n",
        "                msg = '{}\\t{}\\n'.format(key, _format_value(value))\n",
        "                f.write(msg)\n",
        "\n",
        "        print('Saved model evaluation statistics to {}'.format(model_eval_path))\n",
        "\n",
        "\n",
        "class CyclePrinting(object):\n",
        "    def __init__(self, num_epochs, num_batches):\n",
        "        super(CyclePrinting, self).__init__()\n",
        "\n",
        "        self.num_epochs = num_epochs\n",
        "        self.num_batches = num_batches\n",
        "        self.batch_count = 0\n",
        "\n",
        "    def update(self, epoch, metrics):\n",
        "        self.batch_count = (self.batch_count) % self.num_batches + 1\n",
        "\n",
        "        msg = 'epoch {:d}/{:d}, batch {:d}/{:d}'.format(epoch, self.num_epochs,\n",
        "                                                        self.batch_count, self.num_batches)\n",
        "        for key, value in metrics.items():\n",
        "            msg += ', {}: {:4f}'.format(key, value)\n",
        "        print(msg)\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd4VkypYDTcB"
      },
      "source": [
        "## model.py \n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "from torch.distributions import Bernoulli, Categorical\n",
        "\n",
        "\n",
        "class GraphEmbed(nn.Module):\n",
        "    def __init__(self, node_hidden_size):\n",
        "        super(GraphEmbed, self).__init__()\n",
        "\n",
        "        # Setting from the paper\n",
        "        self.graph_hidden_size = 2 * node_hidden_size\n",
        "\n",
        "        # Embed graphs\n",
        "        self.node_gating = nn.Sequential(\n",
        "            nn.Linear(node_hidden_size, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.node_to_graph = nn.Linear(node_hidden_size,\n",
        "                                       self.graph_hidden_size)\n",
        "\n",
        "    def forward(self, g):\n",
        "        if g.number_of_nodes() == 0:\n",
        "            return torch.zeros(1, self.graph_hidden_size)\n",
        "        else:\n",
        "            # Node features are stored as hv in ndata.\n",
        "            hvs = g.ndata['hv']\n",
        "            return (self.node_gating(hvs) *\n",
        "                    self.node_to_graph(hvs)).sum(0, keepdim=True)\n",
        "\n",
        "\n",
        "class GraphProp(nn.Module):\n",
        "    def __init__(self, num_prop_rounds, node_hidden_size):\n",
        "        super(GraphProp, self).__init__()\n",
        "\n",
        "        self.num_prop_rounds = num_prop_rounds\n",
        "\n",
        "        # Setting from the paper\n",
        "        self.node_activation_hidden_size = 2 * node_hidden_size\n",
        "\n",
        "        message_funcs = []\n",
        "        self.reduce_funcs = []\n",
        "        node_update_funcs = []\n",
        "\n",
        "        for t in range(num_prop_rounds):\n",
        "            # input being [hv, hu, xuv]\n",
        "            message_funcs.append(nn.Linear(2 * node_hidden_size + 1,\n",
        "                                           self.node_activation_hidden_size))\n",
        "\n",
        "            self.reduce_funcs.append(partial(self.dgmg_reduce, round=t))\n",
        "            node_update_funcs.append(\n",
        "                nn.GRUCell(self.node_activation_hidden_size,\n",
        "                           node_hidden_size))\n",
        "\n",
        "        self.message_funcs = nn.ModuleList(message_funcs)\n",
        "        self.node_update_funcs = nn.ModuleList(node_update_funcs)\n",
        "\n",
        "    def dgmg_msg(self, edges):\n",
        "        \"\"\"For an edge u->v, return concat([h_u, x_uv])\"\"\"\n",
        "        return {'m': torch.cat([edges.src['hv'],\n",
        "                                edges.data['he']],\n",
        "                               dim=1)}\n",
        "\n",
        "    def dgmg_reduce(self, nodes, round):\n",
        "        hv_old = nodes.data['hv']\n",
        "        m = nodes.mailbox['m']\n",
        "        message = torch.cat([\n",
        "            hv_old.unsqueeze(1).expand(-1, m.size(1), -1), m], dim=2)\n",
        "        node_activation = (self.message_funcs[round](message)).sum(1)\n",
        "\n",
        "        return {'a': node_activation}\n",
        "\n",
        "    def forward(self, g):\n",
        "        if g.number_of_edges() == 0:\n",
        "            return\n",
        "        else:\n",
        "            for t in range(self.num_prop_rounds):\n",
        "                g.update_all(message_func=self.dgmg_msg,\n",
        "                             reduce_func=self.reduce_funcs[t])\n",
        "                g.ndata['hv'] = self.node_update_funcs[t](\n",
        "                    g.ndata['a'], g.ndata['hv'])\n",
        "\n",
        "\n",
        "def bernoulli_action_log_prob(logit, action):\n",
        "    \"\"\"Calculate the log p of an action with respect to a Bernoulli\n",
        "    distribution. Use logit rather than prob for numerical stability.\"\"\"\n",
        "    if action == 0:\n",
        "        return F.logsigmoid(-logit)\n",
        "    else:\n",
        "        return F.logsigmoid(logit)\n",
        "\n",
        "\n",
        "class AddNode(nn.Module):\n",
        "    def __init__(self, graph_embed_func, node_hidden_size):\n",
        "        super(AddNode, self).__init__()\n",
        "\n",
        "        self.graph_op = {'embed': graph_embed_func}\n",
        "\n",
        "        self.stop = 1\n",
        "        self.add_node = nn.Linear(graph_embed_func.graph_hidden_size, 1)\n",
        "\n",
        "        # If to add a node, initialize its hv\n",
        "        self.node_type_embed = nn.Embedding(1, node_hidden_size)\n",
        "        self.initialize_hv = nn.Linear(node_hidden_size + \\\n",
        "                                       graph_embed_func.graph_hidden_size,\n",
        "                                       node_hidden_size)\n",
        "\n",
        "        self.init_node_activation = torch.zeros(1, 2 * node_hidden_size)\n",
        "\n",
        "    def _initialize_node_repr(self, g, node_type, graph_embed):\n",
        "        num_nodes = g.number_of_nodes()\n",
        "        hv_init = self.initialize_hv(\n",
        "            torch.cat([\n",
        "                self.node_type_embed(torch.LongTensor([node_type])),\n",
        "                graph_embed], dim=1))\n",
        "        g.nodes[num_nodes - 1].data['hv'] = hv_init\n",
        "        g.nodes[num_nodes - 1].data['a'] = self.init_node_activation\n",
        "\n",
        "    def prepare_training(self):\n",
        "        self.log_prob = []\n",
        "\n",
        "    def forward(self, g, action=None):\n",
        "        graph_embed = self.graph_op['embed'](g)\n",
        "\n",
        "        logit = self.add_node(graph_embed)\n",
        "        prob = torch.sigmoid(logit)\n",
        "\n",
        "        if not self.training:\n",
        "            action = Bernoulli(prob).sample().item()\n",
        "        stop = bool(action == self.stop)\n",
        "\n",
        "        if not stop:\n",
        "            g.add_nodes(1)\n",
        "            self._initialize_node_repr(g, action, graph_embed)\n",
        "\n",
        "        if self.training:\n",
        "            sample_log_prob = bernoulli_action_log_prob(logit, action)\n",
        "            self.log_prob.append(sample_log_prob)\n",
        "\n",
        "        return stop\n",
        "\n",
        "\n",
        "class AddEdge(nn.Module):\n",
        "    def __init__(self, graph_embed_func, node_hidden_size):\n",
        "        super(AddEdge, self).__init__()\n",
        "\n",
        "        self.graph_op = {'embed': graph_embed_func}\n",
        "        self.add_edge = nn.Linear(graph_embed_func.graph_hidden_size + \\\n",
        "                                  node_hidden_size, 1)\n",
        "\n",
        "    def prepare_training(self):\n",
        "        self.log_prob = []\n",
        "\n",
        "    def forward(self, g, action=None):\n",
        "        graph_embed = self.graph_op['embed'](g)\n",
        "        src_embed = g.nodes[g.number_of_nodes() - 1].data['hv']\n",
        "\n",
        "        logit = self.add_edge(torch.cat(\n",
        "            [graph_embed, src_embed], dim=1))\n",
        "        prob = torch.sigmoid(logit)\n",
        "\n",
        "        if not self.training:\n",
        "            action = Bernoulli(prob).sample().item()\n",
        "        to_add_edge = bool(action == 0)\n",
        "\n",
        "        if self.training:\n",
        "            sample_log_prob = bernoulli_action_log_prob(logit, action)\n",
        "            self.log_prob.append(sample_log_prob)\n",
        "\n",
        "        return to_add_edge\n",
        "\n",
        "\n",
        "class ChooseDestAndUpdate(nn.Module):\n",
        "    def __init__(self, graph_prop_func, node_hidden_size):\n",
        "        super(ChooseDestAndUpdate, self).__init__()\n",
        "\n",
        "        self.graph_op = {'prop': graph_prop_func}\n",
        "        self.choose_dest = nn.Linear(2 * node_hidden_size, 1)\n",
        "\n",
        "    def _initialize_edge_repr(self, g, src_list, dest_list):\n",
        "        # For untyped edges, we only add 1 to indicate its existence.\n",
        "        # For multiple edge types, we can use a one hot representation\n",
        "        # or an embedding module.\n",
        "        edge_repr = torch.ones(len(src_list), 1)\n",
        "        g.edges[src_list, dest_list].data['he'] = edge_repr\n",
        "\n",
        "    def prepare_training(self):\n",
        "        self.log_prob = []\n",
        "\n",
        "    def forward(self, g, dest):\n",
        "        src = g.number_of_nodes() - 1\n",
        "        possible_dests = range(src)\n",
        "\n",
        "        src_embed_expand = g.nodes[src].data['hv'].expand(src, -1)\n",
        "        possible_dests_embed = g.nodes[possible_dests].data['hv']\n",
        "\n",
        "        dests_scores = self.choose_dest(\n",
        "            torch.cat([possible_dests_embed,\n",
        "                       src_embed_expand], dim=1)).view(1, -1)\n",
        "        dests_probs = F.softmax(dests_scores, dim=1)\n",
        "\n",
        "        if not self.training:\n",
        "            dest = Categorical(dests_probs).sample().item()\n",
        "\n",
        "        if not g.has_edge_between(src, dest):\n",
        "            # For undirected graphs, we add edges for both directions\n",
        "            # so that we can perform graph propagation.\n",
        "            src_list = [src, dest]\n",
        "            dest_list = [dest, src]\n",
        "\n",
        "            g.add_edges(src_list, dest_list)\n",
        "            self._initialize_edge_repr(g, src_list, dest_list)\n",
        "\n",
        "            self.graph_op['prop'](g)\n",
        "\n",
        "        if self.training:\n",
        "            if dests_probs.nelement() > 1:\n",
        "                self.log_prob.append(\n",
        "                    F.log_softmax(dests_scores, dim=1)[:, dest: dest + 1])\n",
        "\n",
        "\n",
        "class DGMG(nn.Module):\n",
        "    def __init__(self, v_max, node_hidden_size,\n",
        "                 num_prop_rounds):\n",
        "        super(DGMG, self).__init__()\n",
        "\n",
        "        # Graph configuration\n",
        "        self.v_max = v_max\n",
        "\n",
        "        # Graph embedding module\n",
        "        self.graph_embed = GraphEmbed(node_hidden_size)\n",
        "\n",
        "        # Graph propagation module\n",
        "        self.graph_prop = GraphProp(num_prop_rounds,\n",
        "                                    node_hidden_size)\n",
        "\n",
        "        # Actions\n",
        "        self.add_node_agent = AddNode(\n",
        "            self.graph_embed, node_hidden_size)\n",
        "        self.add_edge_agent = AddEdge(\n",
        "            self.graph_embed, node_hidden_size)\n",
        "        self.choose_dest_agent = ChooseDestAndUpdate(\n",
        "            self.graph_prop, node_hidden_size)\n",
        "\n",
        "        # Weight initialization\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        ##from utils import weights_init, dgmg_message_weight_init\n",
        "\n",
        "        self.graph_embed.apply(weights_init)\n",
        "        self.graph_prop.apply(weights_init)\n",
        "        self.add_node_agent.apply(weights_init)\n",
        "        self.add_edge_agent.apply(weights_init)\n",
        "        self.choose_dest_agent.apply(weights_init)\n",
        "\n",
        "        self.graph_prop.message_funcs.apply(dgmg_message_weight_init)\n",
        "\n",
        "    @property\n",
        "    def action_step(self):\n",
        "        old_step_count = self.step_count\n",
        "        self.step_count += 1\n",
        "\n",
        "        return old_step_count\n",
        "\n",
        "    def prepare_for_train(self):\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.add_node_agent.prepare_training()\n",
        "        self.add_edge_agent.prepare_training()\n",
        "        self.choose_dest_agent.prepare_training()\n",
        "\n",
        "    def add_node_and_update(self, a=None):\n",
        "        \"\"\"Decide if to add a new node.\n",
        "        If a new node should be added, update the graph.\"\"\"\n",
        "\n",
        "        return self.add_node_agent(self.g, a)\n",
        "\n",
        "    def add_edge_or_not(self, a=None):\n",
        "        \"\"\"Decide if a new edge should be added.\"\"\"\n",
        "\n",
        "        return self.add_edge_agent(self.g, a)\n",
        "\n",
        "    def choose_dest_and_update(self, a=None):\n",
        "        \"\"\"Choose destination and connect it to the latest node.\n",
        "        Add edges for both directions and update the graph.\"\"\"\n",
        "\n",
        "        self.choose_dest_agent(self.g, a)\n",
        "\n",
        "    def get_log_prob(self):\n",
        "        return torch.cat(self.add_node_agent.log_prob).sum()\\\n",
        "               + torch.cat(self.add_edge_agent.log_prob).sum()\\\n",
        "               + torch.cat(self.choose_dest_agent.log_prob).sum()\n",
        "\n",
        "    def forward_train(self, actions):\n",
        "        self.prepare_for_train()\n",
        "\n",
        "        stop = self.add_node_and_update(a=actions[self.action_step])\n",
        "\n",
        "        while not stop:\n",
        "            to_add_edge = self.add_edge_or_not(a=actions[self.action_step])\n",
        "            while to_add_edge:\n",
        "                self.choose_dest_and_update(a=actions[self.action_step])\n",
        "                to_add_edge = self.add_edge_or_not(a=actions[self.action_step])\n",
        "            stop = self.add_node_and_update(a=actions[self.action_step])\n",
        "\n",
        "        return self.get_log_prob()\n",
        "\n",
        "    def forward_inference(self):\n",
        "        stop = self.add_node_and_update()\n",
        "        while (not stop) and (self.g.number_of_nodes() < self.v_max + 1):\n",
        "            num_trials = 0\n",
        "            to_add_edge = self.add_edge_or_not()\n",
        "            while to_add_edge and (num_trials < self.g.number_of_nodes() - 1):\n",
        "                self.choose_dest_and_update()\n",
        "                num_trials += 1\n",
        "                to_add_edge = self.add_edge_or_not()\n",
        "            stop = self.add_node_and_update()\n",
        "\n",
        "        return self.g\n",
        "\n",
        "    def forward(self, actions=None):\n",
        "        # The graph we will work on\n",
        "        self.g = dgl.DGLGraph()\n",
        "\n",
        "        # If there are some features for nodes and edges,\n",
        "        # zero tensors will be set for those of new nodes and edges.\n",
        "        self.g.set_n_initializer(dgl.frame.zero_initializer)\n",
        "        self.g.set_e_initializer(dgl.frame.zero_initializer)\n",
        "\n",
        "        if self.training:\n",
        "            return self.forward_train(actions)\n",
        "        else:\n",
        "            return self.forward_inference()\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnqae5JEDzBH"
      },
      "source": [
        "# utils.py \n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "#                                                    configuration                                                     #\n",
        "########################################################################################################################\n",
        "\n",
        "def mkdir_p(path):\n",
        "    import errno\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "        print('Created directory {}'.format(path))\n",
        "    except OSError as exc:\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            print('Directory {} already exists.'.format(path))\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "def date_filename(base_dir='./'):\n",
        "    dt = datetime.datetime.now()\n",
        "    return os.path.join(base_dir, '{}_{:02d}-{:02d}-{:02d}'.format(\n",
        "        dt.date(), dt.hour, dt.minute, dt.second\n",
        "    ))\n",
        "\n",
        "def setup_log_dir(opts):\n",
        "    log_dir = '{}'.format(date_filename(opts['log_dir']))\n",
        "    mkdir_p(log_dir)\n",
        "    return log_dir\n",
        "\n",
        "def save_arg_dict(opts, filename='settings.txt'):\n",
        "    def _format_value(v):\n",
        "        if isinstance(v, float):\n",
        "            return '{:.4f}'.format(v)\n",
        "        elif isinstance(v, int):\n",
        "            return '{:d}'.format(v)\n",
        "        else:\n",
        "            return '{}'.format(v)\n",
        "\n",
        "    save_path = os.path.join(opts['log_dir'], filename)\n",
        "    with open(save_path, 'w') as f:\n",
        "        for key, value in opts.items():\n",
        "            f.write('{}\\t{}\\n'.format(key, _format_value(value)))\n",
        "    print('Saved settings to {}'.format(save_path))\n",
        "\n",
        "def setup(args):\n",
        "    opts = args.copy()\n",
        "\n",
        "    cudnn.benchmark = False\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "    # Seed\n",
        "    if opts['seed'] is None:\n",
        "        opts['seed'] = random.randint(1, 10000)\n",
        "    random.seed(opts['seed'])\n",
        "    torch.manual_seed(opts['seed'])\n",
        "\n",
        "    # Dataset\n",
        "    #from configure import dataset_based_configure\n",
        "    opts = dataset_based_configure(opts)\n",
        "\n",
        "    assert opts['path_to_dataset'] is not None, 'Expect path to dataset to be set.'\n",
        "    if not os.path.exists(opts['path_to_dataset']):\n",
        "        if opts['dataset'] == 'cycles':\n",
        "            #from cycles import generate_dataset\n",
        "            generate_dataset(opts['min_size'], opts['max_size'],\n",
        "                             opts['ds_size'], opts['path_to_dataset'])\n",
        "        else:\n",
        "            raise ValueError('Unsupported dataset: {}'.format(opts['dataset']))\n",
        "\n",
        "    # Optimization\n",
        "    if opts['clip_grad']:\n",
        "        assert opts['clip_grad'] is not None, 'Expect the gradient norm constraint to be set.'\n",
        "\n",
        "    # Log\n",
        "    print('Prepare logging directory...')\n",
        "    log_dir = setup_log_dir(opts)\n",
        "    opts['log_dir'] = log_dir\n",
        "    mkdir_p(log_dir + '/samples')\n",
        "\n",
        "    plt.switch_backend('Agg')\n",
        "\n",
        "    save_arg_dict(opts)\n",
        "    pprint(opts)\n",
        "\n",
        "    return opts\n",
        "\n",
        "########################################################################################################################\n",
        "#                                                         model                                                        #\n",
        "########################################################################################################################\n",
        "\n",
        "def weights_init(m):\n",
        "    '''\n",
        "    Code from https://gist.github.com/jeasinema/ed9236ce743c8efaf30fa2ff732749f5\n",
        "    Usage:\n",
        "        model = Model()\n",
        "        model.apply(weight_init)\n",
        "    '''\n",
        "    if isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.GRUCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "\n",
        "def dgmg_message_weight_init(m):\n",
        "    \"\"\"\n",
        "    This is similar as the function above where we initialize linear layers from a normal distribution with std\n",
        "    1./10 as suggested by the author. This should only be used for the message passing functions, i.e. fe's in the\n",
        "    paper.\n",
        "    \"\"\"\n",
        "    def _weight_init(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            init.normal_(m.weight.data, std=1./10)\n",
        "            init.normal_(m.bias.data, std=1./10)\n",
        "        else:\n",
        "            raise ValueError('Expected the input to be of type nn.Linear!')\n",
        "\n",
        "    if isinstance(m, nn.ModuleList):\n",
        "        for layer in m:\n",
        "            layer.apply(_weight_init)\n",
        "    else:\n",
        "        m.apply(_weight_init)\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YP1dYBeO0LT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nvkvm287BUA"
      },
      "source": [
        "\"\"\"\n",
        "Learning Deep Generative Models of Graphs\n",
        "Paper: https://arxiv.org/pdf/1803.03324.pdf\n",
        "\n",
        "This implementation works with a minibatch of size 1 only for both training and inference.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import datetime\n",
        "import time\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "#from model import DGMG\n",
        "\n",
        "\n",
        "def main(opts):\n",
        "    print(opts)\n",
        "    t1 = time.time()\n",
        "\n",
        "    # Setup dataset and data loader\n",
        "    if opts['dataset'] == 'cycles':\n",
        "       # from cycles import CycleDataset, CycleModelEvaluation, CyclePrinting\n",
        "       \n",
        "        dataset = CycleDataset(fname=opts['path_to_dataset'])\n",
        "        evaluator = CycleModelEvaluation(v_min=opts['min_size'],\n",
        "                                         v_max=opts['max_size'],\n",
        "                                         dir=opts['log_dir'])\n",
        "        printer = CyclePrinting(num_epochs=opts['nepochs'],\n",
        "                                num_batches=opts['ds_size'] // opts['batch_size'])\n",
        "    else:\n",
        "        raise ValueError('Unsupported dataset: {}'.format(opts['dataset']))\n",
        "\n",
        "    data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0,\n",
        "                             collate_fn=dataset.collate_single)\n",
        "\n",
        "    # Initialize_model\n",
        "    model = DGMG(v_max=opts['max_size'],\n",
        "                 node_hidden_size=opts['node_hidden_size'],\n",
        "                 num_prop_rounds=opts['num_propagation_rounds'])\n",
        "\n",
        "    # Initialize optimizer\n",
        "    if opts['optimizer'] == 'Adam':\n",
        "        optimizer = Adam(model.parameters(), lr=opts['lr'])\n",
        "    else:\n",
        "        raise ValueError('Unsupported argument for the optimizer')\n",
        "\n",
        "    t2 = time.time()\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for epoch in range(opts['nepochs']):\n",
        "        batch_count = 0\n",
        "        batch_loss = 0\n",
        "        batch_prob = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, data in enumerate(data_loader):\n",
        "\n",
        "            log_prob = model(actions=data)\n",
        "            prob = log_prob.detach().exp()\n",
        "\n",
        "            loss = - log_prob / opts['batch_size']\n",
        "            prob_averaged = prob / opts['batch_size']\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            batch_loss += loss.item()\n",
        "            batch_prob += prob_averaged.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            if batch_count % opts['batch_size'] == 0:\n",
        "                printer.update(epoch + 1, {'averaged_loss': batch_loss,\n",
        "                                           'averaged_prob': batch_prob})\n",
        "\n",
        "                if opts['clip_grad']:\n",
        "                    clip_grad_norm_(model.parameters(), opts['clip_bound'])\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                batch_loss = 0\n",
        "                batch_prob = 0\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "    t3 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "    evaluator.rollout_and_examine(model, opts['num_generated_samples'])\n",
        "    evaluator.write_summary()\n",
        "\n",
        "    t4 = time.time()\n",
        "\n",
        "    print('It took {} to setup.'.format(datetime.timedelta(seconds=t2-t1)))\n",
        "    print('It took {} to finish training.'.format(datetime.timedelta(seconds=t3-t2)))\n",
        "    print('It took {} to finish evaluation.'.format(datetime.timedelta(seconds=t4-t3)))\n",
        "    print('--------------------------------------------------------------------------')\n",
        "    print('On average, an epoch takes {}.'.format(datetime.timedelta(\n",
        "        seconds=(t3-t2) / opts['nepochs'])))\n",
        "\n",
        "    del model.g\n",
        "    torch.save(model, './model.pth')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xoFxz_97gis",
        "outputId": "1a871907-f136-416f-9bf5-7914cd37475c"
      },
      "source": [
        "args={\"seed\":9284,\"dataset\":'cycles',\"path_to_dataset\":'./db/cycle.p',\"log_dir\":'./results',\"batch_size\":1,\"clip_grad\":True,\"clip_bound\":0.25}\n",
        "  \n",
        "opts = setup(args)\n",
        "#opts\n",
        "#print(opts['path-to-dataset'])\n",
        "generate_dataset(10, 20, 10, './db/cycle.p')\n",
        "main(opts)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prepare logging directory...\n",
            "Created directory ./results/2021-04-04_20-00-41\n",
            "Created directory ./results/2021-04-04_20-00-41/samples\n",
            "Saved settings to ./results/2021-04-04_20-00-41/settings.txt\n",
            "{'batch_size': 1,\n",
            " 'clip_bound': 0.25,\n",
            " 'clip_grad': True,\n",
            " 'dataset': 'cycles',\n",
            " 'ds_size': 4000,\n",
            " 'log_dir': './results/2021-04-04_20-00-41',\n",
            " 'lr': 0.0005,\n",
            " 'max_size': 20,\n",
            " 'min_size': 10,\n",
            " 'nepochs': 25,\n",
            " 'node_hidden_size': 16,\n",
            " 'num_generated_samples': 100,\n",
            " 'num_propagation_rounds': 3,\n",
            " 'optimizer': 'Adam',\n",
            " 'path_to_dataset': './db/cycle.p',\n",
            " 'seed': 9284}\n",
            "17\n",
            "12\n",
            "20\n",
            "18\n",
            "14\n",
            "15\n",
            "18\n",
            "12\n",
            "16\n",
            "14\n",
            "{'seed': 9284, 'dataset': 'cycles', 'path_to_dataset': './db/cycle.p', 'log_dir': './results/2021-04-04_20-00-41', 'batch_size': 1, 'clip_grad': True, 'clip_bound': 0.25, 'node_hidden_size': 16, 'num_propagation_rounds': 3, 'optimizer': 'Adam', 'nepochs': 25, 'ds_size': 4000, 'num_generated_samples': 100, 'min_size': 10, 'max_size': 20, 'lr': 0.0005}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n",
            "/usr/local/lib/python3.7/dist-packages/dgl/base.py:45: DGLWarning: DGLGraph.has_edge_between is deprecated. Please use DGLGraph.has_edges_between\n",
            "  return warnings.warn(message, category=category, stacklevel=1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1/25, batch 1/4000, averaged_loss: 58.793091, averaged_prob: 0.000000\n",
            "epoch 1/25, batch 2/4000, averaged_loss: 53.490570, averaged_prob: 0.000000\n",
            "epoch 1/25, batch 3/4000, averaged_loss: 44.949020, averaged_prob: 0.000000\n",
            "epoch 1/25, batch 4/4000, averaged_loss: 40.529678, averaged_prob: 0.000000\n",
            "epoch 1/25, batch 5/4000, averaged_loss: 53.506821, averaged_prob: 0.000000\n",
            "epoch 1/25, batch 6/4000, averaged_loss: 45.454678, averaged_prob: 0.000000\n",
            "epoch 1/25, batch 7/4000, averaged_loss: 38.039959, averaged_prob: 0.000000\n",
            "epoch 1/25, batch 8/4000, averaged_loss: 31.244255, averaged_prob: 0.000000\n",
            "epoch 1/25, batch 9/4000, averaged_loss: 55.237202, averaged_prob: 0.000000\n",
            "epoch 1/25, batch 10/4000, averaged_loss: 29.991196, averaged_prob: 0.000000\n",
            "epoch 2/25, batch 11/4000, averaged_loss: 43.473442, averaged_prob: 0.000000\n",
            "epoch 2/25, batch 12/4000, averaged_loss: 45.301174, averaged_prob: 0.000000\n",
            "epoch 2/25, batch 13/4000, averaged_loss: 35.919403, averaged_prob: 0.000000\n",
            "epoch 2/25, batch 14/4000, averaged_loss: 32.488171, averaged_prob: 0.000000\n",
            "epoch 2/25, batch 15/4000, averaged_loss: 26.912067, averaged_prob: 0.000000\n",
            "epoch 2/25, batch 16/4000, averaged_loss: 40.724091, averaged_prob: 0.000000\n",
            "epoch 2/25, batch 17/4000, averaged_loss: 25.699501, averaged_prob: 0.000000\n",
            "epoch 2/25, batch 18/4000, averaged_loss: 43.248070, averaged_prob: 0.000000\n",
            "epoch 2/25, batch 19/4000, averaged_loss: 28.640316, averaged_prob: 0.000000\n",
            "epoch 2/25, batch 20/4000, averaged_loss: 32.024014, averaged_prob: 0.000000\n",
            "epoch 3/25, batch 21/4000, averaged_loss: 39.465839, averaged_prob: 0.000000\n",
            "epoch 3/25, batch 22/4000, averaged_loss: 26.488590, averaged_prob: 0.000000\n",
            "epoch 3/25, batch 23/4000, averaged_loss: 22.308332, averaged_prob: 0.000000\n",
            "epoch 3/25, batch 24/4000, averaged_loss: 32.270844, averaged_prob: 0.000000\n",
            "epoch 3/25, batch 25/4000, averaged_loss: 24.546402, averaged_prob: 0.000000\n",
            "epoch 3/25, batch 26/4000, averaged_loss: 28.803297, averaged_prob: 0.000000\n",
            "epoch 3/25, batch 27/4000, averaged_loss: 29.628122, averaged_prob: 0.000000\n",
            "epoch 3/25, batch 28/4000, averaged_loss: 24.289909, averaged_prob: 0.000000\n",
            "epoch 3/25, batch 29/4000, averaged_loss: 25.124119, averaged_prob: 0.000000\n",
            "epoch 3/25, batch 30/4000, averaged_loss: 19.185343, averaged_prob: 0.000000\n",
            "epoch 4/25, batch 31/4000, averaged_loss: 21.318680, averaged_prob: 0.000000\n",
            "epoch 4/25, batch 32/4000, averaged_loss: 25.875156, averaged_prob: 0.000000\n",
            "epoch 4/25, batch 33/4000, averaged_loss: 22.782337, averaged_prob: 0.000000\n",
            "epoch 4/25, batch 34/4000, averaged_loss: 19.993082, averaged_prob: 0.000000\n",
            "epoch 4/25, batch 35/4000, averaged_loss: 17.476173, averaged_prob: 0.000000\n",
            "epoch 4/25, batch 36/4000, averaged_loss: 17.176937, averaged_prob: 0.000000\n",
            "epoch 4/25, batch 37/4000, averaged_loss: 21.824593, averaged_prob: 0.000000\n",
            "epoch 4/25, batch 38/4000, averaged_loss: 19.422482, averaged_prob: 0.000000\n",
            "epoch 4/25, batch 39/4000, averaged_loss: 23.697517, averaged_prob: 0.000000\n",
            "epoch 4/25, batch 40/4000, averaged_loss: 21.325615, averaged_prob: 0.000000\n",
            "epoch 5/25, batch 41/4000, averaged_loss: 20.862854, averaged_prob: 0.000000\n",
            "epoch 5/25, batch 42/4000, averaged_loss: 20.420313, averaged_prob: 0.000000\n",
            "epoch 5/25, batch 43/4000, averaged_loss: 16.922415, averaged_prob: 0.000000\n",
            "epoch 5/25, batch 44/4000, averaged_loss: 16.648245, averaged_prob: 0.000000\n",
            "epoch 5/25, batch 45/4000, averaged_loss: 17.787399, averaged_prob: 0.000000\n",
            "epoch 5/25, batch 46/4000, averaged_loss: 18.148882, averaged_prob: 0.000000\n",
            "epoch 5/25, batch 47/4000, averaged_loss: 16.523924, averaged_prob: 0.000000\n",
            "epoch 5/25, batch 48/4000, averaged_loss: 19.357418, averaged_prob: 0.000000\n",
            "epoch 5/25, batch 49/4000, averaged_loss: 14.211162, averaged_prob: 0.000001\n",
            "epoch 5/25, batch 50/4000, averaged_loss: 14.026270, averaged_prob: 0.000001\n",
            "epoch 6/25, batch 51/4000, averaged_loss: 13.841816, averaged_prob: 0.000001\n",
            "epoch 6/25, batch 52/4000, averaged_loss: 16.803391, averaged_prob: 0.000000\n",
            "epoch 6/25, batch 53/4000, averaged_loss: 15.994190, averaged_prob: 0.000000\n",
            "epoch 6/25, batch 54/4000, averaged_loss: 14.752600, averaged_prob: 0.000000\n",
            "epoch 6/25, batch 55/4000, averaged_loss: 14.051391, averaged_prob: 0.000001\n",
            "epoch 6/25, batch 56/4000, averaged_loss: 12.939091, averaged_prob: 0.000002\n",
            "epoch 6/25, batch 57/4000, averaged_loss: 16.107138, averaged_prob: 0.000000\n",
            "epoch 6/25, batch 58/4000, averaged_loss: 14.212185, averaged_prob: 0.000001\n",
            "epoch 6/25, batch 59/4000, averaged_loss: 14.723322, averaged_prob: 0.000000\n",
            "epoch 6/25, batch 60/4000, averaged_loss: 13.003811, averaged_prob: 0.000002\n",
            "epoch 7/25, batch 61/4000, averaged_loss: 12.093478, averaged_prob: 0.000006\n",
            "epoch 7/25, batch 62/4000, averaged_loss: 12.619614, averaged_prob: 0.000003\n",
            "epoch 7/25, batch 63/4000, averaged_loss: 13.072433, averaged_prob: 0.000002\n",
            "epoch 7/25, batch 64/4000, averaged_loss: 12.257455, averaged_prob: 0.000005\n",
            "epoch 7/25, batch 65/4000, averaged_loss: 13.233167, averaged_prob: 0.000002\n",
            "epoch 7/25, batch 66/4000, averaged_loss: 13.543141, averaged_prob: 0.000001\n",
            "epoch 7/25, batch 67/4000, averaged_loss: 12.802522, averaged_prob: 0.000003\n",
            "epoch 7/25, batch 68/4000, averaged_loss: 12.355715, averaged_prob: 0.000004\n",
            "epoch 7/25, batch 69/4000, averaged_loss: 11.699344, averaged_prob: 0.000008\n",
            "epoch 7/25, batch 70/4000, averaged_loss: 10.841921, averaged_prob: 0.000020\n",
            "epoch 8/25, batch 71/4000, averaged_loss: 12.043887, averaged_prob: 0.000006\n",
            "epoch 8/25, batch 72/4000, averaged_loss: 10.600566, averaged_prob: 0.000025\n",
            "epoch 8/25, batch 73/4000, averaged_loss: 11.291946, averaged_prob: 0.000012\n",
            "epoch 8/25, batch 74/4000, averaged_loss: 10.357261, averaged_prob: 0.000032\n",
            "epoch 8/25, batch 75/4000, averaged_loss: 10.615284, averaged_prob: 0.000025\n",
            "epoch 8/25, batch 76/4000, averaged_loss: 11.171568, averaged_prob: 0.000014\n",
            "epoch 8/25, batch 77/4000, averaged_loss: 10.840266, averaged_prob: 0.000020\n",
            "epoch 8/25, batch 78/4000, averaged_loss: 10.186234, averaged_prob: 0.000038\n",
            "epoch 8/25, batch 79/4000, averaged_loss: 10.972775, averaged_prob: 0.000017\n",
            "epoch 8/25, batch 80/4000, averaged_loss: 10.042389, averaged_prob: 0.000044\n",
            "epoch 9/25, batch 81/4000, averaged_loss: 9.412168, averaged_prob: 0.000082\n",
            "epoch 9/25, batch 82/4000, averaged_loss: 9.567206, averaged_prob: 0.000070\n",
            "epoch 9/25, batch 83/4000, averaged_loss: 9.850982, averaged_prob: 0.000053\n",
            "epoch 9/25, batch 84/4000, averaged_loss: 8.931158, averaged_prob: 0.000132\n",
            "epoch 9/25, batch 85/4000, averaged_loss: 9.972749, averaged_prob: 0.000047\n",
            "epoch 9/25, batch 86/4000, averaged_loss: 8.909805, averaged_prob: 0.000135\n",
            "epoch 9/25, batch 87/4000, averaged_loss: 8.889666, averaged_prob: 0.000138\n",
            "epoch 9/25, batch 88/4000, averaged_loss: 8.858598, averaged_prob: 0.000142\n",
            "epoch 9/25, batch 89/4000, averaged_loss: 8.957181, averaged_prob: 0.000129\n",
            "epoch 9/25, batch 90/4000, averaged_loss: 8.757805, averaged_prob: 0.000157\n",
            "epoch 10/25, batch 91/4000, averaged_loss: 8.042871, averaged_prob: 0.000321\n",
            "epoch 10/25, batch 92/4000, averaged_loss: 7.973476, averaged_prob: 0.000344\n",
            "epoch 10/25, batch 93/4000, averaged_loss: 8.150751, averaged_prob: 0.000289\n",
            "epoch 10/25, batch 94/4000, averaged_loss: 8.257077, averaged_prob: 0.000259\n",
            "epoch 10/25, batch 95/4000, averaged_loss: 7.276240, averaged_prob: 0.000692\n",
            "epoch 10/25, batch 96/4000, averaged_loss: 7.445779, averaged_prob: 0.000584\n",
            "epoch 10/25, batch 97/4000, averaged_loss: 6.817580, averaged_prob: 0.001094\n",
            "epoch 10/25, batch 98/4000, averaged_loss: 7.376542, averaged_prob: 0.000626\n",
            "epoch 10/25, batch 99/4000, averaged_loss: 6.524675, averaged_prob: 0.001467\n",
            "epoch 10/25, batch 100/4000, averaged_loss: 6.803720, averaged_prob: 0.001110\n",
            "epoch 11/25, batch 101/4000, averaged_loss: 6.672182, averaged_prob: 0.001266\n",
            "epoch 11/25, batch 102/4000, averaged_loss: 6.192288, averaged_prob: 0.002045\n",
            "epoch 11/25, batch 103/4000, averaged_loss: 6.148586, averaged_prob: 0.002137\n",
            "epoch 11/25, batch 104/4000, averaged_loss: 6.017357, averaged_prob: 0.002436\n",
            "epoch 11/25, batch 105/4000, averaged_loss: 5.702954, averaged_prob: 0.003336\n",
            "epoch 11/25, batch 106/4000, averaged_loss: 6.181769, averaged_prob: 0.002067\n",
            "epoch 11/25, batch 107/4000, averaged_loss: 7.547786, averaged_prob: 0.000527\n",
            "epoch 11/25, batch 108/4000, averaged_loss: 7.502002, averaged_prob: 0.000552\n",
            "epoch 11/25, batch 109/4000, averaged_loss: 6.748549, averaged_prob: 0.001173\n",
            "epoch 11/25, batch 110/4000, averaged_loss: 7.638997, averaged_prob: 0.000481\n",
            "epoch 12/25, batch 111/4000, averaged_loss: 5.452104, averaged_prob: 0.004287\n",
            "epoch 12/25, batch 112/4000, averaged_loss: 5.507476, averaged_prob: 0.004056\n",
            "epoch 12/25, batch 113/4000, averaged_loss: 5.918158, averaged_prob: 0.002690\n",
            "epoch 12/25, batch 114/4000, averaged_loss: 5.634808, averaged_prob: 0.003571\n",
            "epoch 12/25, batch 115/4000, averaged_loss: 5.444621, averaged_prob: 0.004319\n",
            "epoch 12/25, batch 116/4000, averaged_loss: 5.850188, averaged_prob: 0.002879\n",
            "epoch 12/25, batch 117/4000, averaged_loss: 5.518539, averaged_prob: 0.004012\n",
            "epoch 12/25, batch 118/4000, averaged_loss: 5.890966, averaged_prob: 0.002764\n",
            "epoch 12/25, batch 119/4000, averaged_loss: 5.493432, averaged_prob: 0.004114\n",
            "epoch 12/25, batch 120/4000, averaged_loss: 6.756110, averaged_prob: 0.001164\n",
            "epoch 13/25, batch 121/4000, averaged_loss: 5.056316, averaged_prob: 0.006369\n",
            "epoch 13/25, batch 122/4000, averaged_loss: 5.449004, averaged_prob: 0.004301\n",
            "epoch 13/25, batch 123/4000, averaged_loss: 5.522637, averaged_prob: 0.003995\n",
            "epoch 13/25, batch 124/4000, averaged_loss: 5.678823, averaged_prob: 0.003418\n",
            "epoch 13/25, batch 125/4000, averaged_loss: 5.056012, averaged_prob: 0.006371\n",
            "epoch 13/25, batch 126/4000, averaged_loss: 5.857328, averaged_prob: 0.002859\n",
            "epoch 13/25, batch 127/4000, averaged_loss: 5.529625, averaged_prob: 0.003967\n",
            "epoch 13/25, batch 128/4000, averaged_loss: 5.866281, averaged_prob: 0.002833\n",
            "epoch 13/25, batch 129/4000, averaged_loss: 7.496946, averaged_prob: 0.000555\n",
            "epoch 13/25, batch 130/4000, averaged_loss: 4.733450, averaged_prob: 0.008796\n",
            "epoch 14/25, batch 131/4000, averaged_loss: 4.439840, averaged_prob: 0.011798\n",
            "epoch 14/25, batch 132/4000, averaged_loss: 4.684580, averaged_prob: 0.009237\n",
            "epoch 14/25, batch 133/4000, averaged_loss: 7.511205, averaged_prob: 0.000547\n",
            "epoch 14/25, batch 134/4000, averaged_loss: 4.329863, averaged_prob: 0.013169\n",
            "epoch 14/25, batch 135/4000, averaged_loss: 4.513293, averaged_prob: 0.010962\n",
            "epoch 14/25, batch 136/4000, averaged_loss: 5.162248, averaged_prob: 0.005729\n",
            "epoch 14/25, batch 137/4000, averaged_loss: 5.161539, averaged_prob: 0.005733\n",
            "epoch 14/25, batch 138/4000, averaged_loss: 4.595079, averaged_prob: 0.010101\n",
            "epoch 14/25, batch 139/4000, averaged_loss: 4.551943, averaged_prob: 0.010547\n",
            "epoch 14/25, batch 140/4000, averaged_loss: 5.943057, averaged_prob: 0.002624\n",
            "epoch 15/25, batch 141/4000, averaged_loss: 5.474429, averaged_prob: 0.004193\n",
            "epoch 15/25, batch 142/4000, averaged_loss: 4.394917, averaged_prob: 0.012340\n",
            "epoch 15/25, batch 143/4000, averaged_loss: 4.403398, averaged_prob: 0.012236\n",
            "epoch 15/25, batch 144/4000, averaged_loss: 4.274548, averaged_prob: 0.013918\n",
            "epoch 15/25, batch 145/4000, averaged_loss: 4.892809, averaged_prob: 0.007500\n",
            "epoch 15/25, batch 146/4000, averaged_loss: 5.932718, averaged_prob: 0.002651\n",
            "epoch 15/25, batch 147/4000, averaged_loss: 4.017838, averaged_prob: 0.017992\n",
            "epoch 15/25, batch 148/4000, averaged_loss: 3.517247, averaged_prob: 0.029681\n",
            "epoch 15/25, batch 149/4000, averaged_loss: 10.407209, averaged_prob: 0.000030\n",
            "epoch 15/25, batch 150/4000, averaged_loss: 8.574383, averaged_prob: 0.000189\n",
            "epoch 16/25, batch 151/4000, averaged_loss: 7.353998, averaged_prob: 0.000640\n",
            "epoch 16/25, batch 152/4000, averaged_loss: 6.155549, averaged_prob: 0.002122\n",
            "epoch 16/25, batch 153/4000, averaged_loss: 5.142312, averaged_prob: 0.005844\n",
            "epoch 16/25, batch 154/4000, averaged_loss: 6.201076, averaged_prob: 0.002027\n",
            "epoch 16/25, batch 155/4000, averaged_loss: 5.649343, averaged_prob: 0.003520\n",
            "epoch 16/25, batch 156/4000, averaged_loss: 6.030070, averaged_prob: 0.002405\n",
            "epoch 16/25, batch 157/4000, averaged_loss: 4.345759, averaged_prob: 0.012962\n",
            "epoch 16/25, batch 158/4000, averaged_loss: 4.547241, averaged_prob: 0.010596\n",
            "epoch 16/25, batch 159/4000, averaged_loss: 5.359416, averaged_prob: 0.004704\n",
            "epoch 16/25, batch 160/4000, averaged_loss: 5.369277, averaged_prob: 0.004657\n",
            "epoch 17/25, batch 161/4000, averaged_loss: 5.084661, averaged_prob: 0.006191\n",
            "epoch 17/25, batch 162/4000, averaged_loss: 4.761918, averaged_prob: 0.008549\n",
            "epoch 17/25, batch 163/4000, averaged_loss: 5.150311, averaged_prob: 0.005798\n",
            "epoch 17/25, batch 164/4000, averaged_loss: 4.425321, averaged_prob: 0.011970\n",
            "epoch 17/25, batch 165/4000, averaged_loss: 4.622358, averaged_prob: 0.009830\n",
            "epoch 17/25, batch 166/4000, averaged_loss: 4.400666, averaged_prob: 0.012269\n",
            "epoch 17/25, batch 167/4000, averaged_loss: 4.564653, averaged_prob: 0.010413\n",
            "epoch 17/25, batch 168/4000, averaged_loss: 3.956065, averaged_prob: 0.019138\n",
            "epoch 17/25, batch 169/4000, averaged_loss: 4.060832, averaged_prob: 0.017235\n",
            "epoch 17/25, batch 170/4000, averaged_loss: 3.667957, averaged_prob: 0.025529\n",
            "epoch 18/25, batch 171/4000, averaged_loss: 3.783393, averaged_prob: 0.022745\n",
            "epoch 18/25, batch 172/4000, averaged_loss: 4.390053, averaged_prob: 0.012400\n",
            "epoch 18/25, batch 173/4000, averaged_loss: 6.295631, averaged_prob: 0.001844\n",
            "epoch 18/25, batch 174/4000, averaged_loss: 6.451269, averaged_prob: 0.001579\n",
            "epoch 18/25, batch 175/4000, averaged_loss: 3.405766, averaged_prob: 0.033181\n",
            "epoch 18/25, batch 176/4000, averaged_loss: 3.283392, averaged_prob: 0.037501\n",
            "epoch 18/25, batch 177/4000, averaged_loss: 8.742389, averaged_prob: 0.000160\n",
            "epoch 18/25, batch 178/4000, averaged_loss: 3.136451, averaged_prob: 0.043437\n",
            "epoch 18/25, batch 179/4000, averaged_loss: 4.340837, averaged_prob: 0.013026\n",
            "epoch 18/25, batch 180/4000, averaged_loss: 3.115151, averaged_prob: 0.044372\n",
            "epoch 19/25, batch 181/4000, averaged_loss: 4.322322, averaged_prob: 0.013269\n",
            "epoch 19/25, batch 182/4000, averaged_loss: 4.137365, averaged_prob: 0.015965\n",
            "epoch 19/25, batch 183/4000, averaged_loss: 3.088756, averaged_prob: 0.045559\n",
            "epoch 19/25, batch 184/4000, averaged_loss: 4.499049, averaged_prob: 0.011120\n",
            "epoch 19/25, batch 185/4000, averaged_loss: 5.808928, averaged_prob: 0.003001\n",
            "epoch 19/25, batch 186/4000, averaged_loss: 3.737257, averaged_prob: 0.023819\n",
            "epoch 19/25, batch 187/4000, averaged_loss: 5.839770, averaged_prob: 0.002910\n",
            "epoch 19/25, batch 188/4000, averaged_loss: 5.369490, averaged_prob: 0.004657\n",
            "epoch 19/25, batch 189/4000, averaged_loss: 4.874128, averaged_prob: 0.007642\n",
            "epoch 19/25, batch 190/4000, averaged_loss: 4.112939, averaged_prob: 0.016360\n",
            "epoch 20/25, batch 191/4000, averaged_loss: 3.118415, averaged_prob: 0.044227\n",
            "epoch 20/25, batch 192/4000, averaged_loss: 4.063176, averaged_prob: 0.017194\n",
            "epoch 20/25, batch 193/4000, averaged_loss: 3.289742, averaged_prob: 0.037263\n",
            "epoch 20/25, batch 194/4000, averaged_loss: 3.502685, averaged_prob: 0.030116\n",
            "epoch 20/25, batch 195/4000, averaged_loss: 2.890738, averaged_prob: 0.055535\n",
            "epoch 20/25, batch 196/4000, averaged_loss: 3.579248, averaged_prob: 0.027897\n",
            "epoch 20/25, batch 197/4000, averaged_loss: 17.898800, averaged_prob: 0.000000\n",
            "epoch 20/25, batch 198/4000, averaged_loss: 13.709100, averaged_prob: 0.000001\n",
            "epoch 20/25, batch 199/4000, averaged_loss: 8.136883, averaged_prob: 0.000293\n",
            "epoch 20/25, batch 200/4000, averaged_loss: 2.606206, averaged_prob: 0.073814\n",
            "epoch 21/25, batch 201/4000, averaged_loss: 4.045741, averaged_prob: 0.017497\n",
            "epoch 21/25, batch 202/4000, averaged_loss: 11.583591, averaged_prob: 0.000009\n",
            "epoch 21/25, batch 203/4000, averaged_loss: 8.062331, averaged_prob: 0.000315\n",
            "epoch 21/25, batch 204/4000, averaged_loss: 2.881205, averaged_prob: 0.056067\n",
            "epoch 21/25, batch 205/4000, averaged_loss: 4.766414, averaged_prob: 0.008511\n",
            "epoch 21/25, batch 206/4000, averaged_loss: 3.158035, averaged_prob: 0.042509\n",
            "epoch 21/25, batch 207/4000, averaged_loss: 2.964829, averaged_prob: 0.051569\n",
            "epoch 21/25, batch 208/4000, averaged_loss: 5.618833, averaged_prob: 0.003629\n",
            "epoch 21/25, batch 209/4000, averaged_loss: 3.109542, averaged_prob: 0.044621\n",
            "epoch 21/25, batch 210/4000, averaged_loss: 6.137829, averaged_prob: 0.002160\n",
            "epoch 22/25, batch 211/4000, averaged_loss: 4.158843, averaged_prob: 0.015626\n",
            "epoch 22/25, batch 212/4000, averaged_loss: 4.585422, averaged_prob: 0.010199\n",
            "epoch 22/25, batch 213/4000, averaged_loss: 3.934782, averaged_prob: 0.019550\n",
            "epoch 22/25, batch 214/4000, averaged_loss: 3.427464, averaged_prob: 0.032469\n",
            "epoch 22/25, batch 215/4000, averaged_loss: 3.446080, averaged_prob: 0.031870\n",
            "epoch 22/25, batch 216/4000, averaged_loss: 4.131543, averaged_prob: 0.016058\n",
            "epoch 22/25, batch 217/4000, averaged_loss: 3.324687, averaged_prob: 0.035984\n",
            "epoch 22/25, batch 218/4000, averaged_loss: 3.430068, averaged_prob: 0.032385\n",
            "epoch 22/25, batch 219/4000, averaged_loss: 4.436476, averaged_prob: 0.011838\n",
            "epoch 22/25, batch 220/4000, averaged_loss: 2.937190, averaged_prob: 0.053014\n",
            "epoch 23/25, batch 221/4000, averaged_loss: 3.560842, averaged_prob: 0.028415\n",
            "epoch 23/25, batch 222/4000, averaged_loss: 3.748577, averaged_prob: 0.023551\n",
            "epoch 23/25, batch 223/4000, averaged_loss: 3.432285, averaged_prob: 0.032313\n",
            "epoch 23/25, batch 224/4000, averaged_loss: 2.675615, averaged_prob: 0.068864\n",
            "epoch 23/25, batch 225/4000, averaged_loss: 2.630937, averaged_prob: 0.072011\n",
            "epoch 23/25, batch 226/4000, averaged_loss: 3.429245, averaged_prob: 0.032411\n",
            "epoch 23/25, batch 227/4000, averaged_loss: 5.050583, averaged_prob: 0.006406\n",
            "epoch 23/25, batch 228/4000, averaged_loss: 6.893959, averaged_prob: 0.001014\n",
            "epoch 23/25, batch 229/4000, averaged_loss: 2.627295, averaged_prob: 0.072274\n",
            "epoch 23/25, batch 230/4000, averaged_loss: 4.008658, averaged_prob: 0.018158\n",
            "epoch 24/25, batch 231/4000, averaged_loss: 2.638024, averaged_prob: 0.071502\n",
            "epoch 24/25, batch 232/4000, averaged_loss: 4.286437, averaged_prob: 0.013754\n",
            "epoch 24/25, batch 233/4000, averaged_loss: 4.938640, averaged_prob: 0.007164\n",
            "epoch 24/25, batch 234/4000, averaged_loss: 2.791828, averaged_prob: 0.061309\n",
            "epoch 24/25, batch 235/4000, averaged_loss: 5.049571, averaged_prob: 0.006412\n",
            "epoch 24/25, batch 236/4000, averaged_loss: 4.547256, averaged_prob: 0.010596\n",
            "epoch 24/25, batch 237/4000, averaged_loss: 3.285998, averaged_prob: 0.037403\n",
            "epoch 24/25, batch 238/4000, averaged_loss: 2.643024, averaged_prob: 0.071146\n",
            "epoch 24/25, batch 239/4000, averaged_loss: 3.906762, averaged_prob: 0.020105\n",
            "epoch 24/25, batch 240/4000, averaged_loss: 2.935440, averaged_prob: 0.053107\n",
            "epoch 25/25, batch 241/4000, averaged_loss: 3.380147, averaged_prob: 0.034042\n",
            "epoch 25/25, batch 242/4000, averaged_loss: 5.337626, averaged_prob: 0.004807\n",
            "epoch 25/25, batch 243/4000, averaged_loss: 8.538318, averaged_prob: 0.000196\n",
            "epoch 25/25, batch 244/4000, averaged_loss: 2.274220, averaged_prob: 0.102877\n",
            "epoch 25/25, batch 245/4000, averaged_loss: 3.126962, averaged_prob: 0.043851\n",
            "epoch 25/25, batch 246/4000, averaged_loss: 4.371303, averaged_prob: 0.012635\n",
            "epoch 25/25, batch 247/4000, averaged_loss: 2.532707, averaged_prob: 0.079444\n",
            "epoch 25/25, batch 248/4000, averaged_loss: 7.322711, averaged_prob: 0.000660\n",
            "epoch 25/25, batch 249/4000, averaged_loss: 3.132627, averaged_prob: 0.043603\n",
            "epoch 25/25, batch 250/4000, averaged_loss: 4.015932, averaged_prob: 0.018026\n",
            "Saved model evaluation statistics to ./results/2021-04-04_20-00-41/model_eval.txt\n",
            "It took 0:00:00.011501 to setup.\n",
            "It took 0:00:49.370377 to finish training.\n",
            "It took 0:00:18.817645 to finish evaluation.\n",
            "--------------------------------------------------------------------------\n",
            "On average, an epoch takes 0:00:01.974815.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNO8p6Bt81ZA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}